<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Max Weissenbacher" />

<meta name="date" content="2023-01-11" />

<title>Particle filters for stochastic volatility</title>

<script src="libs/header-attrs-2.17/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="libs/navigation-1.1/tabsets.js"></script>
<link href="libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div id="header">



<h1 class="title toc-ignore">Particle filters for stochastic
volatility</h1>
<h4 class="author">Max Weissenbacher</h4>
<h4 class="date">2023-01-11</h4>

</div>


<p>In this document we simulate a stochastic volatility hidden Markov
model and demonstrate our implementation of</p>
<ul>
<li>Sequential Importance Sampling (<code>SIS</code>),</li>
<li>Sequential Importance Resampling (<code>SIR</code>),</li>
<li>Resample Move Particle Filter (<code>RMPF</code>).</li>
</ul>
<p>After simulating data for the hidden Markov model, we apply the three
filtering algorithms, assess their performance and discuss some of the
associated theory.</p>
<div id="introduction" class="section level3">
<h3>Introduction</h3>
<p>We consider a hidden Markov model for stochastic volatility. The
model is comprised of a Markov chain <span
class="math inline">\(X_n\)</span> modeling the volatility of an asset
and the observed returns of the asset <span
class="math inline">\(Y_n\)</span>. The model is defined as follows:
<span class="math display">\[
\begin{align}
X_n &amp;= \rho X_{n-1} + \sigma V_n, &amp;V_n \sim N(0,1)\\
Y_n &amp;= \tau \exp \left( \frac{X_n}{2} \right) W_n, &amp;W_n \sim
N(0,1)
\end{align}
\]</span> for constants <span class="math inline">\(\rho, \tau, \sigma
\in \mathbb{R}\)</span>. We assume that <span class="math inline">\(X_0
\sim N \left(0, \frac{\sigma^2}{1-\rho^2} \right)\)</span>. Note that
<span class="math inline">\(V_n,W_n\)</span> are noise terms.</p>
<p>Our task is to infer the <em>unobservable</em> or <em>hidden</em>
state <span class="math inline">\(X_n\)</span> from the history of
observations <span class="math inline">\(Y_0, \dots, Y_n\)</span>.
Expressed in a more precise form, the <strong>filtering problem</strong>
consists in estimating <span class="math display">\[
\mathbb{P}(X_n \in \cdot \mid Y_0, \dots, Y_n).
\]</span> Equivalently, we could ask about estimating the expectation
<span class="math display">\[
\mathbb{E}(\varphi(x_n) \mid y_0, \dots y_n) = \int \varphi(x_n) \, p(d
x_n \mid y_0, \dots y_n)
\]</span> for any (sufficiently regular) function <span
class="math inline">\(\varphi(x_n)\)</span>.</p>
</div>
<div id="simulating-the-hidden-markov-model" class="section level3">
<h3>Simulating the hidden Markov model</h3>
<p>We begin by simulating the hidden states <span
class="math inline">\(X_n\)</span> and observations <span
class="math inline">\(Y_n\)</span>. Below, we will then infer the state
<span class="math inline">\(X_n\)</span> using only knowledge of the
observations <span class="math inline">\(Y\)</span> and the parameters
of the model. We choose the parameters</p>
<p><span class="math display">\[
\rho = 0.9, \quad \sigma = 1.05, \quad \tau = 0.7.
\]</span> Running a simulation of the hidden and observable states
yields the following trajectories:
<img src="particle-filters_files/figure-html/Simulation-1.png" width="672" /></p>
<p>We store the resulting time series in the vectors <code>X</code>
(hidden states) respectively <code>Y</code> (observed states).</p>
</div>
<div id="sequential-importance-re-sampling" class="section level3">
<h3>Sequential Importance (Re-)Sampling</h3>
<p>We implement sequential importance sampling (<code>SIS</code>),
sequential importance resampling (<code>SIR</code>) and a resample move
particle filter (<code>RMPF</code>). The algorithms are sequential
versions of importance sampling and can be used for <em>online</em>
inference in hidden Markov models.</p>
<p>For each time step <span class="math inline">\(n\)</span>, we
simulate a population of <span class="math inline">\(P\)</span>
particles <span class="math inline">\(X^{(i)}_n, i = 1, \dots P\)</span>
together with associated weights <span
class="math inline">\(W^{(i)}_n\)</span>. The weights satisfy <span
class="math inline">\(\sum_{i=1}^P W^{(i)}_n = 1\)</span>. To estimate
the expectation of a function <span
class="math inline">\(\varphi(x_n)\)</span>, given that we have observed
<span class="math inline">\(y_0, \dots y_n\)</span>, we can use the
following formula: <span class="math display">\[
\begin{align}
\mathbb{E}(\varphi(x_n) \mid y_0, \dots y_n) &amp;= \int \varphi(x_n) \,
p(d x_n \mid y_0, \dots y_n) \\
&amp;\approx \sum_{i=1}^P \varphi(X_n^{(i)}) W^{(i)}_n.
\end{align}
\]</span> A detailed description of the
<code>SIR</code>/<code>SIS</code> algorithms can be found in this <a
href="https://en.wikipedia.org/wiki/Particle_filter#Sequential_Importance_Resampling_(SIR)">Wikipedia
article on particle filters</a>. For an explanation of the resample move
particle filter, see the <a
href="https://github.com/nkantas/LTCC-Advanced-Computational-Methods-in-Statistics">Advanced
Computational Methods in Statistics course</a> by <a
href="https://www.ma.imperial.ac.uk/~nkantas/">Nikolas Kantas</a>.</p>
<p>The function <code>SequentialImportanceSampling</code> accepts the
observed data <code>Y</code>, the model parameters, as well as several
hyperparameters. All three algorithms use the bootstrap proposal. The
MCMC (Markov chain Monte Carlo) algorithm used for the resampling move
is Random walk Metropolis Hastings. The implementation of the algorithms
is contained in the <code>particlefilters.R</code> file, which is also
part of this repository.</p>
<pre class="r"><code>N &lt;- 700 # number of particles
SIS &lt;- SequentialImportanceSampling(data = Y,
                                    params = params,
                                    num_particles = N)
SIR &lt;- SequentialImportanceSampling(data = Y,
                                    params = params,
                                    num_particles = N,
                                    resample = TRUE)
RMPF &lt;- SequentialImportanceSampling(data = Y,
                                     params = params,
                                     num_particles = N,
                                     resample = TRUE,
                                     move = TRUE,
                                     rsmpl_steps = 5,
                                     stepSize = 4)</code></pre>
<p><img src="particle-filters_files/figure-html/Plotting%20means-1.png" width="672" /><img src="particle-filters_files/figure-html/Plotting%20means-2.png" width="672" /></p>
<p>We compute the root mean squared error for each algorithm to be</p>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">SIS</th>
<th align="right">SIR</th>
<th align="right">RMPF</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Root mean squared error</td>
<td align="right">3.002253</td>
<td align="right">1.066643</td>
<td align="right">1.085281</td>
</tr>
</tbody>
</table>
</div>
<div id="fine-tuning-of-mcmc-step-size" class="section level3">
<h3>Fine tuning of MCMC step size</h3>
<p>The step size of the resampling move needs to be finetuned to ensure
the acceptance ratio of the Metropolis Hastings algorithm is in a good
range (between <span class="math inline">\(0.2\)</span> and <span
class="math inline">\(0.4\)</span>). Here we adjusted the step size
manually for the given model parameters. In practice, conducting a
hyperparameter search might be advantageous. For the current run we
obtained a mean acceptance ratio of 0.24 with variance 0.00046.</p>
</div>
<div id="weight-degeneracy" class="section level3">
<h3>Weight degeneracy</h3>
<p>We observe that the <code>SIS</code> algorithm tracks the true state
well for a few (~ 30) time steps and after that it diverts significantly
from the true hidden state. This may be explained by <em>weight
degeneracy</em>: as <span class="math inline">\(n\)</span> grows, the
weights <span class="math inline">\(W^{(i)}_n\)</span> of most particles
tend to <span class="math inline">\(0\)</span>, while a few particles
have weights very close to <span class="math inline">\(1\)</span>. This
effectively renders most of the simulated particles ineffective. The
<code>SIR</code> algorithm addresses this issue by taking inspiration
from genetic algorithms and resampling particles according to the
distribution of weights.</p>
<p>We can assess the effect of weight degeneracy using <em>effective
sample size</em>. It is defined in terms of the weights at time <span
class="math inline">\(n\)</span> as <span class="math display">\[
\text{ESS}_n = \left( \sum_{i=1}^P \left(W^{(i)}_n\right)^2 \right)^{-1}
\in [1,P]
\]</span> An effective sample size of <span
class="math inline">\(P\)</span> (the total number of particles used in
the simulation) indicates that the algorithm performs as well as perfect
Monte Carlo, while an effective sample size close to <span
class="math inline">\(1\)</span> suggests poor performance. Below we
plot <span class="math inline">\(\text{ESS}_n\)</span> for all three
algorithms.</p>
<p><img src="particle-filters_files/figure-html/Plotting%20effective%20sample%20size-1.png" width="672" /></p>
<p>The mean effective sample size for each of the algorithms is</p>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">SIS</th>
<th align="right">SIR</th>
<th align="right">RMPF</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Effective sample size (rounded)</td>
<td align="right">2</td>
<td align="right">452</td>
<td align="right">452</td>
</tr>
<tr class="even">
<td align="left">Number of particles in simulation</td>
<td align="right">700</td>
<td align="right">700</td>
<td align="right">700</td>
</tr>
</tbody>
</table>
</div>
<div id="path-degeneracy" class="section level3">
<h3>Path degeneracy</h3>
<p>Resampling in the <code>SIR</code> algorithm introduces the issue of
<em>path degeneracy</em>. This effectively means that marginals of the
form <span class="math inline">\(p(x_{n-L}:n \mid y_{0}, \dots y_n
)\)</span> for small <span class="math inline">\(L\)</span> will be well
approximated at the expense of losing particle diversity for early
times. To counteract path degeneracy, the <code>RMPF</code> algorithm
moves each particle <span class="math inline">\(X^{(i)}\)</span> by
using a few iterations of a Markov chain Monte Carlo (MCMC) algorithm
targeting the correct density. This re-introduces particle
diversity.</p>
</div>
<div id="monte-carlo-variance" class="section level3">
<h3>Monte Carlo variance</h3>
<p>Finally, we compute the Monte Carlo variance for the mean for each of
the three algorithms. We simulate multiple runs of each algorithm and
compute the variance of the estimated mean as a function of time.</p>
<p><img src="particle-filters_files/figure-html/Monte%20Carlo%20Variance-1.png" width="672" /></p>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
